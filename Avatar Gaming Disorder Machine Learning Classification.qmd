---
title: "Classification Gaming Disorder Avatar Machine Learning"
format: html
editor: visual
---

Running the packages library

```{r}
echo=FALSE
warning=FALSE
error=FALSE
library(tidyverse)
library(tidymodels)
library(purrr)
library(haven)
library(scales)
library(ggpubr)
library(patchwork)
library(psych)
library(rstatix)
library(rcompanion)
library(broom)
library(knitr)
library(readr)
library(here)
library(psych)
library(officer)
library(ggplot2)
library(dplyr)
library(broom.mixed) # for converting bayesian models to tidy tibbles
library(dotwhisker)  # for visualizing regression results
library(rstanarm)
library(agua)
library(parsnip)
library(plyr)
library(themis)
library(DMwR)
library(naivebayes)
library(yardstick)
library(dplyr)
library(vip)
library(palmerpenguins)
library(nnet)
library(Metrics)
library(parsnip)
library(kernlab)
library(e1071)

```

Loading the Data

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
data<-read_sav("C:/Users/vasil/Desktop/R Machine Learning/data.sav")
data1<-data[c("Q37", "Q38", "Q39", "Q40", "Q42", "Q43", "Q44", "Q45", "Q46", "Q47", "Q48", "Q49", "Q50", "Q112", "Q113", "Q114", "Q115", "Q116", "Q117", "Q118", "Q119", "Q120", "Q121", "Q122", "Q123", "Q126", "Q127", "Q128", "Q129", "Q130", "Q131")]
DataNN<-data1%>%setNames(c("GDT1", "GDT2", "GDT3", "GDT4", "IGD1", "IGD2", "IGD3", "IGD4", "IGD5", "IGD6", "IGD7", "IGD8", "IGD9", "ID1", "ID2", "ID3", "ID4", "IM1", "IM2", "IM3", "IM4", "IM5", "COMP1", "COMP2", "COMP3", "PE1", "PE2", "PE3", "PE4", "PE5", "PE6"))
DataN<-DataNN%>%mutate(GDT1=case_when(GDT1>2~1,
                                      GDT1<3~0,
                                      TRUE~NA_real_),
                       GDT2=case_when(GDT2>2~1,
                                      GDT2<3~0,
                                      TRUE~NA_real_),
                       GDT3=case_when(GDT3>2~1,
                                      GDT3<3~0,
                                      TRUE~NA_real_),
                       GDT4=case_when(GDT4>2~1,
                                      GDT4<3~0,
                                      TRUE~NA_real_),
                       IGD1=case_when(IGD1>2~1,
                                      IGD1<3~0,
                                      TRUE~NA_real_),
                       IGD2=case_when(IGD2>2~1,
                                      IGD2<3~0,
                                      TRUE~NA_real_),
                       IGD3=case_when(IGD3>2~1,
                                      IGD3<3~0,
                                      TRUE~NA_real_),
                       IGD4=case_when(IGD4>2~1,
                                      IGD4<3~0,
                                      TRUE~NA_real_),
                       IGD5=case_when(IGD5>2~1,
                                      IGD5<3~0,
                                      TRUE~NA_real_),
                       IGD6=case_when(IGD6>2~1,
                                      IGD6<3~0,
                                      TRUE~NA_real_),
                       IGD7=case_when(IGD7>2~1,
                                      IGD7<3~0,
                                      TRUE~NA_real_),
                       IGD8=case_when(IGD8>2~1,
                                      IGD8<3~0,
                                      TRUE~NA_real_),
                       IGD9=case_when(IGD9>2~1,
                                      IGD9<3~0,
                                      TRUE~NA_real_))
DataM<-DataN%>%mutate(GDTTotal=GDT1+GDT2+GDT3+GDT4)%>%mutate(IGDTTotal=IGD1+IGD2+IGD3+IGD4+IGD5+IGD6+IGD7+IGD8+IGD9)%>%mutate(IDTotal=ID1+ID2+ID3+ID4)%>%mutate(IMTotal=IM1+IM2+IM3+IM4+IM5)%>%mutate(COMPTotal=COMP1+COMP2+COMP3)%>%mutate(PETotal=PE1+PE2+PE3+PE4+PE5+PE6)

DATAAIGD<-DataM[c("GDTTotal", "IDTotal", "IMTotal", "COMPTotal", "PETotal")]
DATAAIIGD<-DataM[c("IGDTTotal", "IDTotal", "IMTotal", "COMPTotal", "PETotal")]
GDTD<-na.omit(DATAAIGD)
IGDD<-na.omit(DATAAIIGD)

GDTD<-GDTD%>%mutate(GDTTotal=case_when(GDTTotal==4~1,
                                       GDTTotal==3~1,
                                      GDTTotal<3~0,
                                      TRUE~NA_real_))
GDTD<-GDTD %>% 
  mutate(GDTTotal = case_when(GDTTotal == 1 ~ "Yes",
                             GDTTotal == 0 ~ "No"))

IGDD<-IGDD%>%mutate(IGDTTotal=case_when(IGDTTotal==9~1,
                                       IGDTTotal==8~1,
                                       IGDTTotal==7~1,
                                       IGDTTotal==6~1,
                                       IGDTTotal==5~1,
                                      IGDTTotal<5~0,
                                      TRUE~NA_real_))
IGDD<-IGDD %>% 
  mutate(IGDTTotal = case_when(IGDTTotal == 1 ~ "Yes",
                             IGDTTotal == 0 ~ "No"))
  GDTDiag<-GDTD%>%setNames(c("GDDiag", "IDTotal", "IMTotal", "COMPTotal", "PETotal")) 
  IGDDiag<-IGDD%>%setNames(c("IGDDiag", "IDTotal", "IMTotal", "COMPTotal", "PETotal"))
  table(GDTDiag$GDDiag)
  table(IGDDiag$IGDDiag)
```

Split the data

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
prior_dist <- rstanarm::student_t(df = 7, location = 0, scale = 2.5)
set.seed(123)
data_split <- initial_split(GDTDiag, prop = 3/4, strata = GDDiag, breaks = 4, pool = 0.1)

# Create data frames for the two sets:
train_data_GD <- training(data_split)
test_data_GD  <- testing(data_split)

#Crossvalidation split
set.seed(123)
folds <- vfold_cv(train_data_GD, v = 10)

table(train_data_GD$GDDiag)
table(test_data_GD$GDDiag)
```

The `echo: false` option disables the printing of code (only output is displayed).

Create the recipe

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
set.seed(123)
GD_rec <- 
  recipe(GDDiag ~ ., data = train_data_GD)%>% step_smote(GDDiag, over_ratio = 0.25)%>%step_dummy(all_nominal_predictors())%>%step_zv(all_predictors())%>% step_normalize(all_numeric_predictors())%>%prep()

train_data_GD_b<-bake(GD_rec, new_data = train_data_GD)
test_data_GD_b<-bake(GD_rec, new_data = test_data_GD)
```

Introduce Models

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
set.seed(123)

## token model
twt_null <- null_model()%>%
  set_engine("parsnip")%>%
  set_mode("classification")


## model specification
lasso_spec <- multinom_reg(penalty = tune(), mixture = 1)%>%
  set_mode("classification")%>%
  set_engine("glmnet")

# model specification
nb_spec <- naive_Bayes()%>%
  set_mode("classification")%>%
  set_engine("naivebayes")

## model spec
#ranger_spec <- rand_forest(mtry = tune(), min_n = tune(), trees = 1000) %>%
  #set_mode("classification") %>%
  #set_engine("ranger")

ranger_spec<-rand_forest() %>% 
  set_engine("ranger", importance = "impurity") %>% 
  set_mode("classification")

##log_regression
logreg_spec <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

##Kernel
svm_spec <- svm_rbf(mode = "classification", 
                    engine = "kernlab",
            cost = 1, rbf_sigma = 0.01)

```

Mixing Recipes and Models to Create Workflows

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
set.seed(123)
#Null
Null_workflow <- workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(twt_null)
#Lasso 
lasso_workflow <- workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(lasso_spec)
#Naive Bayes 
NB_workflow <- workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(nb_spec)
#Random Forests 
RF_workflow <- workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(ranger_spec)
#Log_GLM_Workflow
LOGGLM_workflow <- workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(logreg_spec)
#Kernel
Kernel_workflow<-workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(svm_spec)
```

Fitting Workflows in Training Data

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)

Null_fit<-Null_workflow%>%fit(train_data_GD_b)
Null_fit  
#Lasso_fit<-lasso_workflow%>%fit(train_data_GD_b)
#Lasso_fit
RF_fit<-RF_workflow%>%fit(train_data_GD_b)
RF_fit
LogReg_fit<-LOGGLM_workflow%>%fit(train_data_GD_b)
LogReg_fit
Kernel_fit<-Kernel_workflow%>%fit(train_data_GD_b)
Kernel_fit
#NB_fit<-NB_workflow%>%fit(train_data_GD_b)
#NB_fit
```

Predict Results on the Testing Data Prior Tuning

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
#Null_Model_Untuned
results_NF <- test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols(Null_fit %>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols( Null_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))
#RF
results_RF <- test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols(RF_fit %>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols( RF_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))
#LoReg
results_LR <- test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols (LogReg_fit%>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols( LogReg_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))
#Kernel
results_Kern<-test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols (Kernel_fit%>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols(Kernel_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))

#Lasso
#results_Lasso<-test_data_GD_b %>% select(GDDiag) %>% 
  #bind_cols( Lasso_fit%>% 
              #predict(new_data = test_data_GD_b)) %>% 
  #bind_cols( Lasso_fit%>% 
             # predict(new_data = test_data_GD_b, type = "prob"))

#NB_Fit
#result_NB <- test_data_GD_b %>% select(GDDiag) %>% 
 # bind_cols(NB_fit %>% 
  #            predict(new_data = test_data_GD_b)) %>% 
  #bind_cols( NB_fit%>% 
   #           predict(new_data = test_data_GD_b, type = "prob"))
```

Get the Prediction Fit for its Classification Algorithm Tested

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)
results_NF
eval_metrics_class <- metric_set(ppv, f_meas)
#accuracy(results_NF, GDDiag, .pred_class)
#recall(results_NF, GDDiag, .pred_Yes)

#NF_Results
results_NF %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_NF %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_NF <- results_NF %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_NF <- results_NF %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_NF<-eval_metrics_class(results_NF, truth = GDDiag, estimate = .pred_class)

list(curve_NF, auc_NF, Met_NF)

#Model-specific variable importance scores are currently not available for this type of model.

#RF_Results
results_RF %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_RF %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_RF <- results_RF %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_RF <- results_RF %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_RF<-eval_metrics_class(results_RF, truth = GDDiag, estimate = .pred_class)

list(curve_RF, auc_RF, Met_RF)

RF_fit %>% 
  extract_fit_parsnip() %>% 
 #Make VIP plot
 vip()
# I could not get accuracy and recall can you help?

#LR_Results
results_LR %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_LR %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_LR <- results_LR %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_LR <- results_LR %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_LR<-eval_metrics_class(results_LR, truth = GDDiag, estimate = .pred_class)

list(curve_LR, auc_LR, Met_LR)

LogReg_fit %>% 
  extract_fit_parsnip() %>% 
# Make VIP plot
  vip()

#Kernel_Results
results_Kern %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_Kern %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_kern <- results_Kern %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_kern <- results_Kern %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_kern<-eval_metrics_class(results_Kern, truth = GDDiag, estimate = .pred_class)

list(curve_kern, auc_kern, Met_kern)


#Model-specific variable importance scores are currently not available for this type of model.

```

Tuning tunable algorithms to improve prediction

```{r}
echo=FALSE
error=FALSE
warning=FALSE
options(scipen = 999, digits=3, max.print=999999, show.signif.stars=TRUE)

set.seed(123)

#Kernel
#first creating a tuning model
svm_spec_T <- svm_rbf(mode = "classification", engine = "kernlab", cost = tune(), rbf_sigma = tune())
#second creating a tuning workflow
Kernel_workflow_T<-workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(svm_spec_T)
#third calling the workflow
Kernel_workflow_T

# fourth creating the regular grid of 6 values for each tuning parameters
svm_grid <- grid_regular(parameters(svm_spec_T), levels = 6)
#fifth calling the grid
svm_grid

#sixth tuning the grid
svm_res <- tune_grid(
  object = Kernel_workflow_T,
  resamples = folds,
  grid = svm_grid
)

#Seventh collect tuning metrics
svm_res %>% 
  collect_metrics() %>% 
  slice_head(n = 7)
  show_best()
  
  #Eighth Visualizing Tuning Metrics
  
svm_res %>% 
  collect_metrics() %>% 
  mutate(rbf_sigma = factor(rbf_sigma)) %>% 
  ggplot(mapping = aes(x = cost, y = mean, color = rbf_sigma)) +
  geom_line(size = 1.5, alpha = 0.7) +
  geom_point(size = 2) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "viridis", begin = .1)

#Nineth show the best model
svm_res %>% 
  show_best("accuracy")

# Tenth Select best model hyperparameters
best_svm <- svm_res %>% 
  select_best("accuracy")

#11th Finalize the workoflow with the best SVM

final_wflow <- Kernel_workflow_T %>% 
  finalize_workflow(best_svm)

#12th Check the fit of the final Wflow on the training data
Tuned_Kernel_fit<-final_wflow%>%fit(train_data_GD_b)
Tuned_Kernel_fit
#Test the tuned Kernel on the test data

results_tuned_Kern<-test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols (Tuned_Kernel_fit%>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols(Tuned_Kernel_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))

#Tuned Kernel_Results
results_tuned_Kern %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_tuned_Kern %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_kern_tuned <- results_tuned_Kern %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_kern_tuned <- results_tuned_Kern %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_kern_tuned<-eval_metrics_class(results_tuned_Kern, truth = GDDiag, estimate = .pred_class)

list(curve_kern_tuned, auc_kern_tuned, Met_kern_tuned)


#Tune Random Forests Ranger Engine

set.seed(123)

#RF
#first creating a tuning model
ranger_spec_T <- rand_forest(
  mtry = tune(),
  trees = 1000,
  min_n = tune()
) %>%
  set_mode("classification") %>%
  set_engine("ranger")
#second creating a tuning workflow
RF_workflow_T<-workflow() %>% 
  add_recipe(GD_rec) %>% 
  add_model(ranger_spec_T)
#third calling the workflow
RF_workflow_T

# fourth tuning/testing the workflow to the resamples

set.seed(345)
RF_res <- tune_grid(
  RF_workflow_T,
  resamples = folds,
  grid = 20
)
#Fifth collect and visualize tuning metrics
RF_res %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

#Fifth (laternative) for collecting tuning metrics
RF_res %>% 
  collect_metrics() %>% 
  slice_head(n = 7)

#Nineth show the best model
RF_res %>% 
  show_best("accuracy")

# Tenth Select best model hyperparameters
best_RF <- RF_res %>% 
  select_best("accuracy")

#11th Finalize the workoflow with the best SVM

final_wflow_RF <- RF_workflow_T %>% 
  finalize_workflow(best_RF)

#12th Check the fit of the final Wflow on the training data
Tuned_RF_fit<-final_wflow_RF%>%fit(train_data_GD_b)
Tuned_RF_fit
#Test the tuned Kernel on the test data

results_tuned_RF<-test_data_GD_b %>% select(GDDiag) %>% 
  bind_cols (Tuned_RF_fit%>% 
              predict(new_data = test_data_GD_b)) %>% 
  bind_cols(Tuned_RF_fit%>% 
              predict(new_data = test_data_GD_b, type = "prob"))

#Tuned RF_Results
results_tuned_RF %>% 
  conf_mat(truth = GDDiag, estimate = .pred_class)
#visualise CF
update_geom_defaults(geom = "rect", new = list(fill = "midnightblue", alpha = 0.7))
results_tuned_RF %>% 
  conf_mat(GDDiag, .pred_class) %>% 
  autoplot()
#Plot Roc_Curve
curve_RF_tuned <- results_tuned_RF %>% 
  roc_curve(GDDiag, .pred_Yes) %>% 
  autoplot
# Evaluate ROC_AOC
auc_RF_tuned <- results_tuned_RF %>% 
  roc_auc(GDDiag, .pred_Yes)
#ppv & f_meas
Met_kern_tuned<-eval_metrics_class(results_tuned_RF, truth = GDDiag, estimate = .pred_class)

list(curve_kern_tuned, auc_kern_tuned, Met_kern_tuned)
```
